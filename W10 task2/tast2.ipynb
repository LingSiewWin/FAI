{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, \n",
    "                           roc_auc_score, precision_recall_curve, auc)\n",
    "\n",
    "# Create output directory for saving files\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Dataset successfully loaded with shape: (7043, 21)\n",
      "\n",
      "Preprocessing the data...\n",
      "Categorical columns: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "Numerical columns: ['tenure', 'MonthlyCharges', 'TotalCharges']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/1m9xkbrd2cv9js0zzsrn5gfh0000gn/T/ipykernel_74493/2707667034.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  telco_data['TotalCharges'].fillna(telco_data['TotalCharges'].mean(), inplace=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumerical columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumerical_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Create preprocessors for both types of features\u001b[39;00m\n\u001b[1;32m     37\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     38\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     39\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(), numerical_cols),\n\u001b[0;32m---> 40\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m, categorical_cols)\n\u001b[1;32m     41\u001b[0m     ])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading and Preprocessing\n",
    "# Load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "dataset_path = \"TelcoCustomerChurn.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "try:\n",
    "    telco_data = pd.read_csv(dataset_path)\n",
    "    print(f\"Dataset successfully loaded with shape: {telco_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Data Preprocessing\n",
    "print(\"\\nPreprocessing the data...\")\n",
    "\n",
    "# Convert 'TotalCharges' to numeric, replacing spaces with NaN\n",
    "if telco_data['TotalCharges'].dtype == 'object':\n",
    "    telco_data['TotalCharges'] = pd.to_numeric(telco_data['TotalCharges'], errors='coerce')\n",
    "    telco_data['TotalCharges'].fillna(telco_data['TotalCharges'].mean(), inplace=True)\n",
    "\n",
    "# Convert 'SeniorCitizen' from 0/1 to 'No'/'Yes' for consistent preprocessing\n",
    "telco_data['SeniorCitizen'] = telco_data['SeniorCitizen'].map({0: 'No', 1: 'Yes'})\n",
    "\n",
    "# Separate features and target variable\n",
    "X = telco_data.drop(['customerID', 'Churn'], axis=1)\n",
    "y = telco_data['Churn'].map({'Yes': 1, 'No': 0})  # Convert to binary\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "# Create preprocessors for both types of features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)  # Changed 'sparse' to 'sparse_output'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Class Imbalance Handling\n",
    "# Check for class imbalance\n",
    "print(\"\\nChecking for class imbalance...\")\n",
    "class_counts = y.value_counts()\n",
    "print(\"Target class distribution:\")\n",
    "print(class_counts)\n",
    "print(f\"Class imbalance ratio (majority:minority): {class_counts[0]/class_counts[1]:.2f}:1\")\n",
    "\n",
    "# Split the data into training and testing sets (before handling imbalance)\n",
    "X_train_original, X_test, y_train_original, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply preprocessing to test data\n",
    "X_test_processed = preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Method 1: Use SMOTE for oversampling the minority class\n",
    "print(\"\\nImplementing SMOTE oversampling...\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "X_train_processed_original = preprocessor.transform(X_train_original)\n",
    "\n",
    "# Apply SMOTE to the processed training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_processed_smote, y_train_smote = smote.fit_resample(\n",
    "    X_train_processed_original, y_train_original)\n",
    "\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Method 2: Use class weights to handle imbalance\n",
    "print(\"\\nImplementing class weights...\")\n",
    "# Calculate class weights inversely proportional to class frequencies\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(y_train_original), y=y_train_original)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Training\n",
    "# Train models with different imbalance handling techniques\n",
    "# 1. Baseline model (no imbalance handling)\n",
    "baseline_model = RandomForestClassifier(random_state=42)\n",
    "baseline_model.fit(X_train_processed_original, y_train_original)\n",
    "\n",
    "# 2. SMOTE model\n",
    "smote_model = RandomForestClassifier(random_state=42)\n",
    "smote_model.fit(X_train_processed_smote, y_train_smote)\n",
    "\n",
    "# 3. Class weights model\n",
    "weighted_model = RandomForestClassifier(\n",
    "    random_state=42, class_weight=class_weight_dict)\n",
    "weighted_model.fit(X_train_processed_original, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Custom Evaluation Metrics\n",
    "print(\"\\nImplementing custom evaluation metrics...\")\n",
    "\n",
    "def calculate_business_metrics(y_true, y_pred, y_prob=None, fn_cost=5, fp_cost=1):\n",
    "    \"\"\"\n",
    "    Calculate business-oriented metrics for model evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True class labels\n",
    "    y_pred : array-like\n",
    "        Predicted class labels\n",
    "    y_prob : array-like, optional\n",
    "        Predicted probabilities for the positive class\n",
    "    fn_cost : float, optional\n",
    "        Cost of a false negative (missing a churner)\n",
    "    fp_cost : float, optional\n",
    "        Cost of a false positive (incorrectly predicting churn)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of business metrics\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix elements\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate standard metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate business costs\n",
    "    total_cost = (fn * fn_cost) + (fp * fp_cost)\n",
    "    cost_per_customer = total_cost / len(y_true)\n",
    "    \n",
    "    # Calculate customer retention metrics\n",
    "    retention_rate = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    intervention_efficiency = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Calculate profitability metrics (assuming average values)\n",
    "    avg_customer_value = 1000  # Hypothetical average customer lifetime value\n",
    "    avg_intervention_cost = 100  # Hypothetical cost of retention intervention\n",
    "    \n",
    "    # Potential savings from interventions\n",
    "    potential_savings = tp * avg_customer_value - (tp + fp) * avg_intervention_cost\n",
    "    \n",
    "    # ROI of the churn prevention program\n",
    "    roi = (potential_savings / ((tp + fp) * avg_intervention_cost)) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Return all metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'false_negatives': fn,\n",
    "        'false_positives': fp,\n",
    "        'total_business_cost': total_cost,\n",
    "        'cost_per_customer': cost_per_customer,\n",
    "        'retention_rate': retention_rate,\n",
    "        'intervention_efficiency': intervention_efficiency,\n",
    "        'potential_savings': potential_savings,\n",
    "        'roi': roi\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Custom Evaluation Metrics\n",
    "print(\"\\nImplementing custom evaluation metrics...\")\n",
    "\n",
    "def calculate_business_metrics(y_true, y_pred, y_prob=None, fn_cost=5, fp_cost=1):\n",
    "    \"\"\"\n",
    "    Calculate business-oriented metrics for model evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True class labels\n",
    "    y_pred : array-like\n",
    "        Predicted class labels\n",
    "    y_prob : array-like, optional\n",
    "        Predicted probabilities for the positive class\n",
    "    fn_cost : float, optional\n",
    "        Cost of a false negative (missing a churner)\n",
    "    fp_cost : float, optional\n",
    "        Cost of a false positive (incorrectly predicting churn)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of business metrics\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix elements\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate standard metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate business costs\n",
    "    total_cost = (fn * fn_cost) + (fp * fp_cost)\n",
    "    cost_per_customer = total_cost / len(y_true)\n",
    "    \n",
    "    # Calculate customer retention metrics\n",
    "    retention_rate = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    intervention_efficiency = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Calculate profitability metrics (assuming average values)\n",
    "    avg_customer_value = 1000  # Hypothetical average customer lifetime value\n",
    "    avg_intervention_cost = 100  # Hypothetical cost of retention intervention\n",
    "    \n",
    "    # Potential savings from interventions\n",
    "    potential_savings = tp * avg_customer_value - (tp + fp) * avg_intervention_cost\n",
    "    \n",
    "    # ROI of the churn prevention program\n",
    "    roi = (potential_savings / ((tp + fp) * avg_intervention_cost)) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Return all metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'false_negatives': fn,\n",
    "        'false_positives': fp,\n",
    "        'total_business_cost': total_cost,\n",
    "        'cost_per_customer': cost_per_customer,\n",
    "        'retention_rate': retention_rate,\n",
    "        'intervention_efficiency': intervention_efficiency,\n",
    "        'potential_savings': potential_savings,\n",
    "        'roi': roi\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Threshold Optimization\n",
    "print(\"\\nFinding optimal classification thresholds...\")\n",
    "\n",
    "def find_optimal_thresholds(model, X, y_true, metric_name='f1', fn_cost=5, fp_cost=1):\n",
    "    \"\"\"\n",
    "    Find the optimal classification threshold based on various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        Trained classifier with predict_proba method\n",
    "    X : array-like\n",
    "        Input features\n",
    "    y_true : array-like\n",
    "        True class labels\n",
    "    metric_name : str, optional\n",
    "        Metric to optimize ('f1', 'cost', 'precision', 'recall', 'roi')\n",
    "    fn_cost : float, optional\n",
    "        Cost of a false negative\n",
    "    fp_cost : float, optional\n",
    "        Cost of a false positive\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with optimal thresholds for different metrics\n",
    "    \"\"\"\n",
    "    # Get predicted probabilities\n",
    "    y_probs = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Initialize variables\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    metrics = {\n",
    "        'threshold': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'business_cost': [],\n",
    "        'roi': []\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics for each threshold\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_probs >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Calculate business cost\n",
    "        business_cost = (fn * fn_cost) + (fp * fp_cost)\n",
    "        \n",
    "        # Calculate ROI (simplified)\n",
    "        avg_customer_value = 1000\n",
    "        avg_intervention_cost = 100\n",
    "        potential_savings = tp * avg_customer_value - (tp + fp) * avg_intervention_cost\n",
    "        roi = (potential_savings / ((tp + fp) * avg_intervention_cost)) if (tp + fp) > 0 else 0\n",
    "        \n",
    "        # Store results\n",
    "        metrics['threshold'].append(threshold)\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1'].append(f1)\n",
    "        metrics['business_cost'].append(business_cost)\n",
    "        metrics['roi'].append(roi)\n",
    "    \n",
    "    # Find optimal thresholds\n",
    "    results = {\n",
    "        'accuracy': thresholds[np.argmax(metrics['accuracy'])],\n",
    "        'precision': thresholds[np.argmax(metrics['precision'])],\n",
    "        'recall': thresholds[np.argmax(metrics['recall'])],\n",
    "        'f1': thresholds[np.argmax(metrics['f1'])],\n",
    "        'business_cost': thresholds[np.argmin(metrics['business_cost'])],\n",
    "        'roi': thresholds[np.argmax(metrics['roi'])]\n",
    "    }\n",
    "    \n",
    "    # Return threshold based on specified metric\n",
    "    if metric_name == 'cost':\n",
    "        optimal_threshold = results['business_cost']\n",
    "    else:\n",
    "        optimal_threshold = results[metric_name]\n",
    "    \n",
    "    return {\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'all_thresholds': results,\n",
    "        'metrics': pd.DataFrame(metrics)\n",
    "    }\n",
    "\n",
    "# Find optimal thresholds for each model\n",
    "thresholds_baseline = find_optimal_thresholds(\n",
    "    baseline_model, X_test_processed, y_test, metric_name='f1')\n",
    "thresholds_smote = find_optimal_thresholds(\n",
    "    smote_model, X_test_processed, y_test, metric_name='f1')\n",
    "thresholds_weighted = find_optimal_thresholds(\n",
    "    weighted_model, X_test_processed, y_test, metric_name='f1')\n",
    "\n",
    "# Print optimal thresholds\n",
    "print(\"\\nOptimal Thresholds (F1 Score):\")\n",
    "print(f\"Baseline model: {thresholds_baseline['optimal_threshold']:.4f}\")\n",
    "print(f\"SMOTE model: {thresholds_smote['optimal_threshold']:.4f}\")\n",
    "print(f\"Weighted model: {thresholds_weighted['optimal_threshold']:.4f}\")\n",
    "\n",
    "# Also find optimal thresholds for business cost\n",
    "thresholds_cost_baseline = find_optimal_thresholds(\n",
    "    baseline_model, X_test_processed, y_test, metric_name='cost')\n",
    "thresholds_cost_smote = find_optimal_thresholds(\n",
    "    smote_model, X_test_processed, y_test, metric_name='cost')\n",
    "thresholds_cost_weighted = find_optimal_thresholds(\n",
    "    weighted_model, X_test_processed, y_test, metric_name='cost')\n",
    "\n",
    "print(\"\\nOptimal Thresholds (Business Cost):\")\n",
    "print(f\"Baseline model: {thresholds_cost_baseline['optimal_threshold']:.4f}\")\n",
    "print(f\"SMOTE model: {thresholds_cost_smote['optimal_threshold']:.4f}\")\n",
    "print(f\"Weighted model: {thresholds_cost_weighted['optimal_threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Custom Visualizations\n",
    "# TODO: Create visualizations to understand model performance from a business perspective\n",
    "# Suggestions:\n",
    "# 1. Plot business cost vs. threshold for each model\n",
    "# 2. Plot ROI vs. threshold for each model\n",
    "\n",
    "print(\"\\nCreating custom visualizations...\")\n",
    "\n",
    "# Example placeholder: Plot business cost vs. threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds_baseline['metrics']['threshold'], thresholds_baseline['metrics']['business_cost'], label='Baseline Model')\n",
    "plt.plot(thresholds_smote['metrics']['threshold'], thresholds_smote['metrics']['business_cost'], label='SMOTE Model')\n",
    "plt.plot(thresholds_weighted['metrics']['threshold'], thresholds_weighted['metrics']['business_cost'], label='Weighted Model')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Business Cost')\n",
    "plt.title('Business Cost vs. Classification Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, 'business_cost_vs_threshold.png'))\n",
    "plt.show()\n",
    "\n",
    "# Add more visualizations as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
