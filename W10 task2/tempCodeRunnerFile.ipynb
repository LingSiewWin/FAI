# Cell 3: Class Imbalance Handling
# Check for class imbalance
logger.info("Checking for class imbalance...")
class_counts = y.value_counts()
logger.info("Target class distribution:")
logger.info(f"\n{class_counts}")
logger.info(f"Class imbalance ratio (majority:minority): {class_counts[0]/class_counts[1]:.2f}:1")

# Split the data into training and testing sets
X_train_original, X_test, y_train_original, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Method 1: Use SVMSMOTE for oversampling
logger.info("Implementing SVMSMOTE oversampling...")
# Fit the preprocessor on the training data
preprocessor.fit(X_train_original)
# Transform X_train_original to get the processed features
X_train_processed_original = preprocessor.transform(X_train_original)
# Get the feature names after preprocessing
feature_names = numerical_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))
# Convert to DataFrame for SMOTE
X_train_processed_df = pd.DataFrame(X_train_processed_original, columns=feature_names)
smote = SVMSMOTE(random_state=42)
X_train_processed_smote, y_train_smote = smote.fit_resample(X_train_processed_df, y_train_original)
logger.info("Class distribution after SVMSMOTE:")
logger.info(f"\n{pd.Series(y_train_smote).value_counts()}")

# Method 2: Use class weights
logger.info("Implementing class weights...")
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(
    class_weight='balanced', classes=np.unique(y_train_original), y=y_train_original)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
logger.info(f"Class weights: {class_weight_dict}")